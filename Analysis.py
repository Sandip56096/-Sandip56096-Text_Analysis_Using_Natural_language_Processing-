# -*- coding: utf-8 -*-
"""Text_Analysis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1xypu0-lqVCVzGbVvnJDPRs3omwaO7LsF
"""

#Installing requied libraries used for web scraping
!pip install requests beautifulsoup4

"""Extracting the web article and saving it in a text file with URL_ID as file name"""

import requests
from bs4 import BeautifulSoup

# URL of the article
url = "https://insights.blackcoffer.com/rise-of-telemedicine-and-its-impact-on-livelihood-by-2040-3-2/"

# Send a GET request to the URL
response = requests.get(url)

# Parse the HTML content of the page
soup = BeautifulSoup(response.content, "html.parser")

# Find the article title
title = soup.title.text.strip()

# Find the article text
article_text = ""
for paragraph in soup.find_all("p"):
    article_text += paragraph.text.strip() + "\n"

# Create a text file and write the title and article text to it
file_name = "123.txt"
with open(file_name, "w", encoding="utf-8") as file:
    file.write(f"Title: {title}\n\n")
    file.write(f"Article:\n{article_text}")

print(f"Article content has been extracted and saved to '{file_name}'.")

import nltk
from nltk.corpus import opinion_lexicon

# Download the opinion lexicon if you haven't already
nltk.download('opinion_lexicon')

# Get the list of positive words
positive_words = set(opinion_lexicon.positive())
negative_words = set(opinion_lexicon.negative())

# Print the first 10 positive words as an example
print(list(positive_words)[:10])

# Import the necessary libraries
import re

# Read the text from the file
with open('123.txt', 'r') as file:
    text = file.read()

# Clean the text by removing special characters and numbers, and converting to lowercase
cleaned_text = re.sub(r'[^a-zA-Z\s]', '', text).lower()

# Sample positive and negative word lists (you can use larger lists)
positive_words = set(opinion_lexicon.positive())
negative_words = set(opinion_lexicon.negative())

# Initialize dictionaries to store positive and negative word counts
positive_word_count = {}
negative_word_count = {}

# Count positive words in the cleaned text
for word in positive_words:
    count = cleaned_text.count(word)
    positive_word_count[word] = count

# Count negative words in the cleaned text
for word in negative_words:
    count = cleaned_text.count(word)
    negative_word_count[word] = count

# Print the dictionaries containing positive and negative word counts
print("Positive Word Count:", positive_word_count)
print("Negative Word Count:", negative_word_count)

# Calculate the sum of values in the Positive_word_count dictionary
total_positive_count = sum(positive_word_count.values())

# Calculate the sum of values in the Positive_word_count dictionary
total_negative_count = sum(negative_word_count.values())

# Print the total positive count
print("Total positive count:", total_positive_count)
print("Total negative count:", total_negative_count)

positive_score = total_positive_count
negative_score = total_negative_count

polarity_score = (positive_score-negative_score)/((positive_score+negative_score)+0.000001)
print(polarity_score)

# Split the text into words using space as a delimiter
words = cleaned_text.split()

# Calculate the total number of words
total_words = len(words)

# Print the total number of words
print("Total number of words in the cleaned text:", total_words)

subjectivity_score = (positive_score+negative_score)/((total_words)+0.000001)
print(subjectivity_score)

"""##2. Analysis of Readability"""

import nltk

# Download the punkt tokenizer if you haven't already
nltk.download('punkt')

# Tokenize the text into sentences
sentences = nltk.sent_tokenize(article_text)

# Calculate the number of sentences
num_sentences = len(sentences)

# Print the number of sentences
print("Number of sentences in the text:", num_sentences)

average_sentence_length = total_words/ num_sentences
print(average_sentence_length)

!pip install textstat

import textstat

words = cleaned_text.split()

complex_word_count = 0

for word in words:
    syllable_count = textstat.syllable_count(word)
    # Words with syllable count greater than 2 are considered complex.
    if syllable_count > 2:
        complex_word_count += 1

print("Number of complex words in the cleaned_text:", complex_word_count)

percentage_of_complex_words = complex_word_count/total_words
print(percentage_of_complex_words)

fog_index = 0.4*(average_sentence_length+percentage_of_complex_words)
print(fog_index)

percentage_of_complex_words = (complex_word_count)/(total_words)*100
print(percentage_of_complex_words)

"""print(percentage_of_complex_words)

# 3. Average Number of Words Per Sentence
"""

average_num_of_words_per_sentence = total_words/ num_sentences
print(average_num_of_words_per_sentence)

"""#4. Complex Word Count"""

!pip install textstat

import textstat

words = cleaned_text.split()

complex_word_count = 0

for word in words:
    syllable_count = textstat.syllable_count(word)
    # Words with syllable count greater than 2 are considered complex.
    if syllable_count > 2:
        complex_word_count += 1

print("Number of complex words in the cleaned_text:", complex_word_count)

"""#5. Word Count"""

# Here we have alredy cleaned the file named 123.txt and we have stored the cleaned texts in variable named as clened_text
# so here we will just count the cleaned total words

# Split the text into words using space as a delimiter
words = cleaned_text.split()

# Calculate the total number of words
total_words = len(words)

# Print the total number of words
print("Total number of words in the cleaned text:", total_words)

"""#6. Syllable count per words"""

import textstat

words = cleaned_text.split()

syllable_counts = []

for word in words:
    syllable_count = textstat.syllable_count(word)
    syllable_counts.append(syllable_count)

print("Syllable counts per word in cleaned_text:", syllable_counts)

sum(syllable_counts)

len(syllable_counts)

average_syllabel_count_per_word = sum(syllable_counts)/len(syllable_counts)
print(average_syllabel_count_per_word)

"""#7. Personal Pronoun"""

# List of common personal pronouns
personal_pronouns = ['I', 'you', 'he', 'she', 'it', 'we', 'they', 'me', 'him', 'her', 'us', 'them']

# Split the cleaned_text into words
words = cleaned_text.split()

# Count the number of personal pronouns, excluding "US"
personal_pronoun_count = sum(1 for word in words if word.lower() in personal_pronouns and word.lower() != 'us')

print("Number of personal pronouns in cleaned_text (excluding 'US'):", personal_pronoun_count)

"""#8. Average word length"""

# Split the cleaned_text into words
words = cleaned_text.split()

# Calculate the sum of the total number of characters in each word
total_characters = sum(len(word) for word in words)

print("Sum of total number of characters in each word in cleaned_text:", total_characters)

Average_word_len = total_characters/total_words
print(Average_word_len)

"""#Text Analysis Variable Summary"""

print("Positive Score:",positive_score)
print("Negative Score:",negative_score)
print("Polarity Score:",polarity_score)
print("Subjectivity Score:",subjectivity_score)
print("Average Sentence Length:",average_sentence_length)
print("Percentage of_complex_words:",percentage_of_complex_words)
print("Fog index:",fog_index)
print("Average num of words per sentence",average_num_of_words_per_sentence)
print("Complex word count:",complex_word_count)
print("Word count:",total_words)
print("Syllable per word:",average_syllabel_count_per_word)
print("Personal pronoun count:",personal_pronoun_count)
print("Average word len:",Average_word_len)